# This base image comes shipped with java 11 (needed for scala)
# The openj9 version might be more RAM efficient, but suffers on CPU efficiency
# More details can be found:https://www.royvanrijn.com/blog/2018/05/openj9-jvm-shootout/
# adoptopenjdk on docker hub: https://hub.docker.com/_/adoptopenjdk
# FROM adoptopenjdk:11.0.9_11-jdk-hotspot-focal
# Tag: 11.0.10_9-jdk-hotspot
# Tag: 11.0.11_9-jdk-hotspot-focal
FROM adoptopenjdk:11.0.11_9-jdk-hotspot-focal


LABEL author="Yingding Wang"
LABEL version="0.3"

# Set env variables
ENV DAEMON_RUN=true
ENV SPARK_VERSION=3.1.2
ENV HADOOP_VERSION=3.2
ENV SCALA_VERSION=2.12.14
ENV SCALA_HOME=/usr/share/scala
ENV SPARK_HOME=/spark
ENV SPARK_OPTS="--driver-java-options='-Xms1024M -Xmx4096M -Dlog4j.logLevel=info -Dio.netty.tryReflectionSetAccessible=true'"

# Note: use apt-get clean to clean some cached repository or file, otherwise it will run into error.
RUN apt-get clean && apt-get update && apt-get install -y curl vim wget software-properties-common ssh net-tools ca-certificates iputils-ping inetutils-telnet maven ivy

# Tutorial
# https://github.com/mvillarrealb/docker-spark-cluster/blob/master/docker/base/Dockerfile

RUN cd "/tmp" && \
    wget --no-verbose "https://downloads.typesafe.com/scala/${SCALA_VERSION}/scala-${SCALA_VERSION}.tgz" && \
    tar -xvzf "scala-${SCALA_VERSION}.tgz" && \
    mkdir "${SCALA_HOME}" && \
    rm "/tmp/scala-${SCALA_VERSION}/bin/"*.bat && \
    mv "/tmp/scala-${SCALA_VERSION}/bin" "/tmp/scala-${SCALA_VERSION}/lib" "${SCALA_HOME}" && \
    ln -s "${SCALA_HOME}/bin/"* "/usr/bin/" && \
    rm -rf "/tmp/"*

RUN export PATH="/usr/local/sbt/bin:$PATH" && apt-get update && apt-get install ca-certificates wget tar && mkdir -p "/usr/local/sbt"
# && wget -qO - --no-check-certificate "https://github.com/sbt/sbt/releases/download/v1.2.8/sbt-1.2.8.tgz" | tar xz -C /usr/local/sbt --strip-components=1 && sbt sbtVersion
# RUN export PATH="/usr/local/sbt/bin:$PATH" &&  apk update && apk add ca-certificates wget tar && mkdir -p "/usr/local/sbt"

# Get Apache Spark
# https://www.apache.org/dyn/closer.lua/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
RUN wget --no-verbose https://mirror.dkd.de/apache/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz


# Install Spark and move it to the folder "/spark" and then add this location to the PATH env variable
RUN tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    export PATH=$SPARK_HOME/bin:$PATH


# Handling PID 1 subprocess
# Add Tini
ENV TINI_VERSION v0.19.0
ADD https://github.com/krallin/tini/releases/download/${TINI_VERSION}/tini /tini
RUN chmod +x /tini
